{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [1/469], D_loss: 1.6848, G_loss: 1.6014\n",
      "Epoch [1/40], Step [2/469], D_loss: 8.7140, G_loss: 0.0072\n",
      "Epoch [1/40], Step [3/469], D_loss: 9.1787, G_loss: 0.0048\n",
      "Epoch [1/40], Step [4/469], D_loss: 7.3978, G_loss: 0.0333\n",
      "Epoch [1/40], Step [5/469], D_loss: 6.1375, G_loss: 0.1754\n",
      "Epoch [1/40], Step [6/469], D_loss: 7.0806, G_loss: 0.0867\n",
      "Epoch [1/40], Step [7/469], D_loss: 6.9031, G_loss: 0.0931\n",
      "Epoch [1/40], Step [8/469], D_loss: 6.6143, G_loss: 0.1198\n",
      "Epoch [1/40], Step [9/469], D_loss: 6.9993, G_loss: 0.1516\n",
      "Epoch [1/40], Step [10/469], D_loss: 7.6367, G_loss: 0.0970\n",
      "Epoch [1/40], Step [11/469], D_loss: 7.3100, G_loss: 0.1710\n",
      "Epoch [1/40], Step [12/469], D_loss: 7.8383, G_loss: 0.1200\n",
      "Epoch [1/40], Step [13/469], D_loss: 7.6054, G_loss: 0.2156\n",
      "Epoch [1/40], Step [14/469], D_loss: 8.1108, G_loss: 0.1689\n",
      "Epoch [1/40], Step [15/469], D_loss: 8.1218, G_loss: 0.2812\n",
      "Epoch [1/40], Step [16/469], D_loss: 8.1167, G_loss: 0.2617\n",
      "Epoch [1/40], Step [17/469], D_loss: 8.1437, G_loss: 0.3365\n",
      "Epoch [1/40], Step [18/469], D_loss: 8.4925, G_loss: 0.3019\n",
      "Epoch [1/40], Step [19/469], D_loss: 9.3205, G_loss: 0.2364\n",
      "Epoch [1/40], Step [20/469], D_loss: 9.4041, G_loss: 0.2132\n",
      "Epoch [1/40], Step [21/469], D_loss: 9.1855, G_loss: 0.3115\n",
      "Epoch [1/40], Step [22/469], D_loss: 9.2890, G_loss: 0.2902\n",
      "Epoch [1/40], Step [23/469], D_loss: 9.1763, G_loss: 0.3292\n",
      "Epoch [1/40], Step [24/469], D_loss: 9.0970, G_loss: 0.3972\n",
      "Epoch [1/40], Step [25/469], D_loss: 9.8369, G_loss: 0.2843\n",
      "Epoch [1/40], Step [26/469], D_loss: 10.0794, G_loss: 0.3090\n",
      "Epoch [1/40], Step [27/469], D_loss: 9.5233, G_loss: 0.4440\n",
      "Epoch [1/40], Step [28/469], D_loss: 9.0226, G_loss: 0.6138\n",
      "Epoch [1/40], Step [29/469], D_loss: 8.9425, G_loss: 0.6721\n",
      "Epoch [1/40], Step [30/469], D_loss: 9.4425, G_loss: 0.5675\n",
      "Epoch [1/40], Step [31/469], D_loss: 9.9407, G_loss: 0.3999\n",
      "Epoch [1/40], Step [32/469], D_loss: 9.7037, G_loss: 0.7195\n",
      "Epoch [1/40], Step [33/469], D_loss: 9.7643, G_loss: 0.6393\n",
      "Epoch [1/40], Step [34/469], D_loss: 10.0219, G_loss: 0.5600\n",
      "Epoch [1/40], Step [35/469], D_loss: 9.4472, G_loss: 0.8417\n",
      "Epoch [1/40], Step [36/469], D_loss: 9.3519, G_loss: 0.7287\n",
      "Epoch [1/40], Step [37/469], D_loss: 9.4737, G_loss: 0.8041\n",
      "Epoch [1/40], Step [38/469], D_loss: 9.2433, G_loss: 0.8377\n",
      "Epoch [1/40], Step [39/469], D_loss: 9.3503, G_loss: 0.9291\n",
      "Epoch [1/40], Step [40/469], D_loss: 9.0788, G_loss: 1.1715\n",
      "Epoch [1/40], Step [41/469], D_loss: 8.4651, G_loss: 1.4177\n",
      "Epoch [1/40], Step [42/469], D_loss: 8.1384, G_loss: 1.7978\n",
      "Epoch [1/40], Step [43/469], D_loss: 8.0397, G_loss: 2.0584\n",
      "Epoch [1/40], Step [44/469], D_loss: 8.0421, G_loss: 2.1489\n",
      "Epoch [1/40], Step [45/469], D_loss: 7.4795, G_loss: 2.5648\n",
      "Epoch [1/40], Step [46/469], D_loss: 8.3011, G_loss: 1.7414\n",
      "Epoch [1/40], Step [47/469], D_loss: 8.1873, G_loss: 1.6389\n",
      "Epoch [1/40], Step [48/469], D_loss: 7.8491, G_loss: 2.0457\n",
      "Epoch [1/40], Step [49/469], D_loss: 7.4510, G_loss: 2.2517\n",
      "Epoch [1/40], Step [50/469], D_loss: 8.0117, G_loss: 1.5808\n",
      "Epoch [1/40], Step [51/469], D_loss: 8.2157, G_loss: 1.6402\n",
      "Epoch [1/40], Step [52/469], D_loss: 8.0035, G_loss: 1.7969\n",
      "Epoch [1/40], Step [53/469], D_loss: 7.5715, G_loss: 2.1975\n",
      "Epoch [1/40], Step [54/469], D_loss: 6.5462, G_loss: 2.8300\n",
      "Epoch [1/40], Step [55/469], D_loss: 6.1520, G_loss: 3.4356\n",
      "Epoch [1/40], Step [56/469], D_loss: 5.9569, G_loss: 3.5429\n",
      "Epoch [1/40], Step [57/469], D_loss: 6.1884, G_loss: 2.9756\n",
      "Epoch [1/40], Step [58/469], D_loss: 6.5848, G_loss: 2.7370\n",
      "Epoch [1/40], Step [59/469], D_loss: 6.0842, G_loss: 3.0585\n",
      "Epoch [1/40], Step [60/469], D_loss: 6.8153, G_loss: 2.5654\n",
      "Epoch [1/40], Step [61/469], D_loss: 7.1161, G_loss: 2.1651\n",
      "Epoch [1/40], Step [62/469], D_loss: 6.7850, G_loss: 2.5105\n",
      "Epoch [1/40], Step [63/469], D_loss: 6.8788, G_loss: 2.4871\n",
      "Epoch [1/40], Step [64/469], D_loss: 6.2878, G_loss: 2.9183\n",
      "Epoch [1/40], Step [65/469], D_loss: 6.4861, G_loss: 2.8727\n",
      "Epoch [1/40], Step [66/469], D_loss: 5.9420, G_loss: 3.4843\n",
      "Epoch [1/40], Step [67/469], D_loss: 5.5684, G_loss: 3.8152\n",
      "Epoch [1/40], Step [68/469], D_loss: 5.0971, G_loss: 4.2160\n",
      "Epoch [1/40], Step [69/469], D_loss: 4.5666, G_loss: 4.8878\n",
      "Epoch [1/40], Step [70/469], D_loss: 3.3507, G_loss: 5.7665\n",
      "Epoch [1/40], Step [71/469], D_loss: 3.0519, G_loss: 5.6477\n",
      "Epoch [1/40], Step [72/469], D_loss: 3.8047, G_loss: 5.2649\n",
      "Epoch [1/40], Step [73/469], D_loss: 3.8748, G_loss: 5.0112\n",
      "Epoch [1/40], Step [74/469], D_loss: 3.2553, G_loss: 5.6335\n",
      "Epoch [1/40], Step [75/469], D_loss: 3.6973, G_loss: 5.0627\n",
      "Epoch [1/40], Step [76/469], D_loss: 4.3236, G_loss: 4.2931\n",
      "Epoch [1/40], Step [77/469], D_loss: 5.5491, G_loss: 3.1999\n",
      "Epoch [1/40], Step [78/469], D_loss: 6.3981, G_loss: 2.6153\n",
      "Epoch [1/40], Step [79/469], D_loss: 7.0827, G_loss: 2.0152\n",
      "Epoch [1/40], Step [80/469], D_loss: 7.9338, G_loss: 1.6997\n",
      "Epoch [1/40], Step [81/469], D_loss: 6.9558, G_loss: 2.6348\n",
      "Epoch [1/40], Step [82/469], D_loss: 5.3463, G_loss: 3.6151\n",
      "Epoch [1/40], Step [83/469], D_loss: 5.0225, G_loss: 4.3337\n",
      "Epoch [1/40], Step [84/469], D_loss: 3.6691, G_loss: 5.0171\n",
      "Epoch [1/40], Step [85/469], D_loss: 3.7850, G_loss: 4.3453\n",
      "Epoch [1/40], Step [86/469], D_loss: 5.6600, G_loss: 3.2484\n",
      "Epoch [1/40], Step [87/469], D_loss: 6.4598, G_loss: 2.4694\n",
      "Epoch [1/40], Step [88/469], D_loss: 7.0718, G_loss: 2.4453\n",
      "Epoch [1/40], Step [89/469], D_loss: 6.8359, G_loss: 2.8815\n",
      "Epoch [1/40], Step [90/469], D_loss: 6.2060, G_loss: 3.4796\n",
      "Epoch [1/40], Step [91/469], D_loss: 5.6741, G_loss: 3.3173\n",
      "Epoch [1/40], Step [92/469], D_loss: 6.4451, G_loss: 3.0063\n",
      "Epoch [1/40], Step [93/469], D_loss: 5.7974, G_loss: 3.5059\n",
      "Epoch [1/40], Step [94/469], D_loss: 5.0507, G_loss: 3.7512\n",
      "Epoch [1/40], Step [95/469], D_loss: 5.1422, G_loss: 3.8602\n",
      "Epoch [1/40], Step [96/469], D_loss: 5.5923, G_loss: 3.3977\n",
      "Epoch [1/40], Step [97/469], D_loss: 7.1496, G_loss: 2.1590\n",
      "Epoch [1/40], Step [98/469], D_loss: 6.8204, G_loss: 2.4708\n",
      "Epoch [1/40], Step [99/469], D_loss: 7.3780, G_loss: 2.4154\n",
      "Epoch [1/40], Step [100/469], D_loss: 6.7782, G_loss: 2.7176\n",
      "Epoch [1/40], Step [101/469], D_loss: 6.5802, G_loss: 2.8201\n",
      "Epoch [1/40], Step [102/469], D_loss: 6.2878, G_loss: 3.0083\n",
      "Epoch [1/40], Step [103/469], D_loss: 6.8992, G_loss: 2.6035\n",
      "Epoch [1/40], Step [104/469], D_loss: 7.2223, G_loss: 1.9892\n",
      "Epoch [1/40], Step [105/469], D_loss: 7.6073, G_loss: 2.2633\n",
      "Epoch [1/40], Step [106/469], D_loss: 6.9418, G_loss: 2.9935\n",
      "Epoch [1/40], Step [107/469], D_loss: 5.8219, G_loss: 3.8260\n",
      "Epoch [1/40], Step [108/469], D_loss: 5.4352, G_loss: 3.7997\n",
      "Epoch [1/40], Step [109/469], D_loss: 5.9930, G_loss: 3.8828\n",
      "Epoch [1/40], Step [110/469], D_loss: 5.8187, G_loss: 3.4385\n",
      "Epoch [1/40], Step [111/469], D_loss: 6.3221, G_loss: 3.4752\n",
      "Epoch [1/40], Step [112/469], D_loss: 4.9660, G_loss: 4.6364\n",
      "Epoch [1/40], Step [113/469], D_loss: 3.2571, G_loss: 4.6886\n",
      "Epoch [1/40], Step [114/469], D_loss: 4.5789, G_loss: 4.0101\n",
      "Epoch [1/40], Step [115/469], D_loss: 4.4412, G_loss: 4.0374\n",
      "Epoch [1/40], Step [116/469], D_loss: 4.8306, G_loss: 4.1198\n",
      "Epoch [1/40], Step [117/469], D_loss: 4.4261, G_loss: 4.4723\n",
      "Epoch [1/40], Step [118/469], D_loss: 4.0514, G_loss: 4.6034\n",
      "Epoch [1/40], Step [119/469], D_loss: 3.5444, G_loss: 4.4346\n",
      "Epoch [1/40], Step [120/469], D_loss: 3.3582, G_loss: 4.5944\n",
      "Epoch [1/40], Step [121/469], D_loss: 3.4609, G_loss: 4.3983\n",
      "Epoch [1/40], Step [122/469], D_loss: 4.4294, G_loss: 4.2278\n",
      "Epoch [1/40], Step [123/469], D_loss: 3.8593, G_loss: 4.5104\n",
      "Epoch [1/40], Step [124/469], D_loss: 3.1978, G_loss: 4.8667\n",
      "Epoch [1/40], Step [125/469], D_loss: 2.5824, G_loss: 4.6713\n",
      "Epoch [1/40], Step [126/469], D_loss: 3.2075, G_loss: 4.2752\n",
      "Epoch [1/40], Step [127/469], D_loss: 4.1908, G_loss: 4.0930\n",
      "Epoch [1/40], Step [128/469], D_loss: 3.8471, G_loss: 4.6622\n",
      "Epoch [1/40], Step [129/469], D_loss: 3.6102, G_loss: 5.0749\n",
      "Epoch [1/40], Step [130/469], D_loss: 2.9999, G_loss: 4.9575\n",
      "Epoch [1/40], Step [131/469], D_loss: 2.5592, G_loss: 4.7475\n",
      "Epoch [1/40], Step [132/469], D_loss: 3.0753, G_loss: 4.4191\n",
      "Epoch [1/40], Step [133/469], D_loss: 3.3495, G_loss: 4.4461\n",
      "Epoch [1/40], Step [134/469], D_loss: 2.9165, G_loss: 4.9900\n",
      "Epoch [1/40], Step [135/469], D_loss: 1.7600, G_loss: 4.9528\n",
      "Epoch [1/40], Step [136/469], D_loss: 1.6071, G_loss: 4.5534\n",
      "Epoch [1/40], Step [137/469], D_loss: 1.8309, G_loss: 4.3442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [138/469], D_loss: 1.2605, G_loss: 4.4080\n",
      "Epoch [1/40], Step [139/469], D_loss: 1.3498, G_loss: 4.0295\n",
      "Epoch [1/40], Step [140/469], D_loss: 1.5634, G_loss: 4.4085\n",
      "Epoch [1/40], Step [141/469], D_loss: 1.6456, G_loss: 4.7846\n",
      "Epoch [1/40], Step [142/469], D_loss: 1.1488, G_loss: 4.6242\n",
      "Epoch [1/40], Step [143/469], D_loss: 0.8718, G_loss: 4.1446\n",
      "Epoch [1/40], Step [144/469], D_loss: 1.8600, G_loss: 4.0397\n",
      "Epoch [1/40], Step [145/469], D_loss: 2.9632, G_loss: 4.3334\n",
      "Epoch [1/40], Step [146/469], D_loss: 2.2362, G_loss: 5.6409\n",
      "Epoch [1/40], Step [147/469], D_loss: 1.3971, G_loss: 5.5184\n",
      "Epoch [1/40], Step [148/469], D_loss: 1.3367, G_loss: 4.7973\n",
      "Epoch [1/40], Step [149/469], D_loss: 1.6336, G_loss: 4.3703\n",
      "Epoch [1/40], Step [150/469], D_loss: 1.7321, G_loss: 4.8616\n",
      "Epoch [1/40], Step [151/469], D_loss: 2.5118, G_loss: 5.1771\n",
      "Epoch [1/40], Step [152/469], D_loss: 1.7493, G_loss: 5.6698\n",
      "Epoch [1/40], Step [153/469], D_loss: 0.9481, G_loss: 5.3966\n",
      "Epoch [1/40], Step [154/469], D_loss: 1.6336, G_loss: 5.0342\n",
      "Epoch [1/40], Step [155/469], D_loss: 1.9003, G_loss: 5.1227\n",
      "Epoch [1/40], Step [156/469], D_loss: 1.8976, G_loss: 5.3264\n",
      "Epoch [1/40], Step [157/469], D_loss: 1.4749, G_loss: 5.3269\n",
      "Epoch [1/40], Step [158/469], D_loss: 1.5846, G_loss: 4.9421\n",
      "Epoch [1/40], Step [159/469], D_loss: 1.7722, G_loss: 4.8288\n",
      "Epoch [1/40], Step [160/469], D_loss: 2.1591, G_loss: 5.1874\n",
      "Epoch [1/40], Step [161/469], D_loss: 1.4198, G_loss: 5.6256\n",
      "Epoch [1/40], Step [162/469], D_loss: 1.0660, G_loss: 5.0952\n",
      "Epoch [1/40], Step [163/469], D_loss: 1.1919, G_loss: 4.3685\n",
      "Epoch [1/40], Step [164/469], D_loss: 1.9417, G_loss: 4.3420\n",
      "Epoch [1/40], Step [165/469], D_loss: 1.9430, G_loss: 4.8708\n",
      "Epoch [1/40], Step [166/469], D_loss: 1.3828, G_loss: 5.4110\n",
      "Epoch [1/40], Step [167/469], D_loss: 1.4942, G_loss: 4.9554\n",
      "Epoch [1/40], Step [168/469], D_loss: 1.4772, G_loss: 4.5193\n",
      "Epoch [1/40], Step [169/469], D_loss: 1.7332, G_loss: 4.6329\n",
      "Epoch [1/40], Step [170/469], D_loss: 1.2393, G_loss: 4.8428\n",
      "Epoch [1/40], Step [171/469], D_loss: 1.6777, G_loss: 5.0446\n",
      "Epoch [1/40], Step [172/469], D_loss: 1.1603, G_loss: 4.8885\n",
      "Epoch [1/40], Step [173/469], D_loss: 1.0758, G_loss: 4.5859\n",
      "Epoch [1/40], Step [174/469], D_loss: 1.9099, G_loss: 4.5527\n",
      "Epoch [1/40], Step [175/469], D_loss: 2.1585, G_loss: 5.1038\n",
      "Epoch [1/40], Step [176/469], D_loss: 1.5691, G_loss: 5.3286\n",
      "Epoch [1/40], Step [177/469], D_loss: 1.2387, G_loss: 4.7958\n",
      "Epoch [1/40], Step [178/469], D_loss: 1.2925, G_loss: 4.4339\n",
      "Epoch [1/40], Step [179/469], D_loss: 1.3153, G_loss: 4.4734\n",
      "Epoch [1/40], Step [180/469], D_loss: 1.2850, G_loss: 4.5176\n",
      "Epoch [1/40], Step [181/469], D_loss: 2.0554, G_loss: 4.9443\n",
      "Epoch [1/40], Step [182/469], D_loss: 1.7261, G_loss: 5.7231\n",
      "Epoch [1/40], Step [183/469], D_loss: 1.4058, G_loss: 5.2888\n",
      "Epoch [1/40], Step [184/469], D_loss: 1.4730, G_loss: 4.5164\n",
      "Epoch [1/40], Step [185/469], D_loss: 1.1577, G_loss: 4.2946\n",
      "Epoch [1/40], Step [186/469], D_loss: 1.2458, G_loss: 4.4451\n",
      "Epoch [1/40], Step [187/469], D_loss: 1.6554, G_loss: 4.8505\n",
      "Epoch [1/40], Step [188/469], D_loss: 1.6397, G_loss: 5.0591\n",
      "Epoch [1/40], Step [189/469], D_loss: 1.5313, G_loss: 4.9877\n",
      "Epoch [1/40], Step [190/469], D_loss: 1.0026, G_loss: 4.6986\n",
      "Epoch [1/40], Step [191/469], D_loss: 0.9318, G_loss: 4.1388\n",
      "Epoch [1/40], Step [192/469], D_loss: 2.5538, G_loss: 4.2654\n",
      "Epoch [1/40], Step [193/469], D_loss: 1.9842, G_loss: 5.5758\n",
      "Epoch [1/40], Step [194/469], D_loss: 0.9804, G_loss: 5.5023\n",
      "Epoch [1/40], Step [195/469], D_loss: 0.6713, G_loss: 4.3496\n",
      "Epoch [1/40], Step [196/469], D_loss: 1.2406, G_loss: 3.9339\n",
      "Epoch [1/40], Step [197/469], D_loss: 1.8025, G_loss: 4.5407\n",
      "Epoch [1/40], Step [198/469], D_loss: 1.0953, G_loss: 5.0567\n",
      "Epoch [1/40], Step [199/469], D_loss: 0.7918, G_loss: 4.7383\n",
      "Epoch [1/40], Step [200/469], D_loss: 0.9488, G_loss: 4.2038\n",
      "Epoch [1/40], Step [201/469], D_loss: 1.4240, G_loss: 4.1610\n",
      "Epoch [1/40], Step [202/469], D_loss: 1.1747, G_loss: 4.4451\n",
      "Epoch [1/40], Step [203/469], D_loss: 1.0046, G_loss: 4.6956\n",
      "Epoch [1/40], Step [204/469], D_loss: 1.0213, G_loss: 4.6993\n",
      "Epoch [1/40], Step [205/469], D_loss: 0.8743, G_loss: 4.5742\n",
      "Epoch [1/40], Step [206/469], D_loss: 1.1709, G_loss: 4.4897\n",
      "Epoch [1/40], Step [207/469], D_loss: 1.1579, G_loss: 4.7191\n",
      "Epoch [1/40], Step [208/469], D_loss: 0.7292, G_loss: 4.7714\n",
      "Epoch [1/40], Step [209/469], D_loss: 0.4661, G_loss: 4.0126\n",
      "Epoch [1/40], Step [210/469], D_loss: 1.2928, G_loss: 4.1910\n",
      "Epoch [1/40], Step [211/469], D_loss: 1.0155, G_loss: 4.8949\n",
      "Epoch [1/40], Step [212/469], D_loss: 0.6622, G_loss: 4.7625\n",
      "Epoch [1/40], Step [213/469], D_loss: 0.9904, G_loss: 4.5988\n",
      "Epoch [1/40], Step [214/469], D_loss: 0.7789, G_loss: 4.6640\n",
      "Epoch [1/40], Step [215/469], D_loss: 0.6874, G_loss: 4.4994\n",
      "Epoch [1/40], Step [216/469], D_loss: 0.6425, G_loss: 4.0475\n",
      "Epoch [1/40], Step [217/469], D_loss: 0.8675, G_loss: 4.2604\n",
      "Epoch [1/40], Step [218/469], D_loss: 1.0346, G_loss: 4.6190\n",
      "Epoch [1/40], Step [219/469], D_loss: 0.6776, G_loss: 4.5330\n",
      "Epoch [1/40], Step [220/469], D_loss: 0.6101, G_loss: 4.4269\n",
      "Epoch [1/40], Step [221/469], D_loss: 0.9109, G_loss: 4.3622\n",
      "Epoch [1/40], Step [222/469], D_loss: 0.9547, G_loss: 4.5610\n",
      "Epoch [1/40], Step [223/469], D_loss: 0.6243, G_loss: 4.5241\n",
      "Epoch [1/40], Step [224/469], D_loss: 0.5566, G_loss: 4.2453\n",
      "Epoch [1/40], Step [225/469], D_loss: 0.6680, G_loss: 4.3220\n",
      "Epoch [1/40], Step [226/469], D_loss: 1.0142, G_loss: 4.7962\n",
      "Epoch [1/40], Step [227/469], D_loss: 0.6616, G_loss: 4.7958\n",
      "Epoch [1/40], Step [228/469], D_loss: 0.6949, G_loss: 4.3886\n",
      "Epoch [1/40], Step [229/469], D_loss: 0.8746, G_loss: 4.3180\n",
      "Epoch [1/40], Step [230/469], D_loss: 0.6811, G_loss: 4.5174\n",
      "Epoch [1/40], Step [231/469], D_loss: 0.6046, G_loss: 4.6298\n",
      "Epoch [1/40], Step [232/469], D_loss: 0.5577, G_loss: 4.4569\n",
      "Epoch [1/40], Step [233/469], D_loss: 0.7048, G_loss: 4.4647\n",
      "Epoch [1/40], Step [234/469], D_loss: 0.8173, G_loss: 4.6581\n",
      "Epoch [1/40], Step [235/469], D_loss: 0.4271, G_loss: 4.6857\n",
      "Epoch [1/40], Step [236/469], D_loss: 0.4512, G_loss: 4.4334\n",
      "Epoch [1/40], Step [237/469], D_loss: 0.4589, G_loss: 4.1063\n",
      "Epoch [1/40], Step [238/469], D_loss: 0.8456, G_loss: 4.3674\n",
      "Epoch [1/40], Step [239/469], D_loss: 0.8780, G_loss: 4.8499\n",
      "Epoch [1/40], Step [240/469], D_loss: 0.5592, G_loss: 4.9752\n",
      "Epoch [1/40], Step [241/469], D_loss: 0.5028, G_loss: 4.6757\n",
      "Epoch [1/40], Step [242/469], D_loss: 0.5549, G_loss: 4.3922\n",
      "Epoch [1/40], Step [243/469], D_loss: 0.6916, G_loss: 4.4917\n",
      "Epoch [1/40], Step [244/469], D_loss: 0.5944, G_loss: 4.3866\n",
      "Epoch [1/40], Step [245/469], D_loss: 0.9040, G_loss: 4.5836\n",
      "Epoch [1/40], Step [246/469], D_loss: 0.8478, G_loss: 4.8212\n",
      "Epoch [1/40], Step [247/469], D_loss: 0.6603, G_loss: 4.7868\n",
      "Epoch [1/40], Step [248/469], D_loss: 0.3856, G_loss: 4.4363\n",
      "Epoch [1/40], Step [249/469], D_loss: 0.4887, G_loss: 4.0679\n",
      "Epoch [1/40], Step [250/469], D_loss: 0.8914, G_loss: 4.6364\n",
      "Epoch [1/40], Step [251/469], D_loss: 0.4840, G_loss: 4.9533\n",
      "Epoch [1/40], Step [252/469], D_loss: 0.3711, G_loss: 4.5932\n",
      "Epoch [1/40], Step [253/469], D_loss: 0.3690, G_loss: 4.2621\n",
      "Epoch [1/40], Step [254/469], D_loss: 0.3484, G_loss: 3.7057\n",
      "Epoch [1/40], Step [255/469], D_loss: 1.3159, G_loss: 4.3761\n",
      "Epoch [1/40], Step [256/469], D_loss: 0.6284, G_loss: 5.0885\n",
      "Epoch [1/40], Step [257/469], D_loss: 0.2890, G_loss: 4.8217\n",
      "Epoch [1/40], Step [258/469], D_loss: 0.3833, G_loss: 4.1671\n",
      "Epoch [1/40], Step [259/469], D_loss: 0.6037, G_loss: 3.9404\n",
      "Epoch [1/40], Step [260/469], D_loss: 0.8392, G_loss: 4.7725\n",
      "Epoch [1/40], Step [261/469], D_loss: 0.4226, G_loss: 4.9118\n",
      "Epoch [1/40], Step [262/469], D_loss: 0.3651, G_loss: 4.3374\n",
      "Epoch [1/40], Step [263/469], D_loss: 0.3357, G_loss: 3.8783\n",
      "Epoch [1/40], Step [264/469], D_loss: 0.6516, G_loss: 4.3741\n",
      "Epoch [1/40], Step [265/469], D_loss: 0.8118, G_loss: 4.9247\n",
      "Epoch [1/40], Step [266/469], D_loss: 0.5173, G_loss: 4.8670\n",
      "Epoch [1/40], Step [267/469], D_loss: 0.4861, G_loss: 4.3723\n",
      "Epoch [1/40], Step [268/469], D_loss: 0.5471, G_loss: 4.2217\n",
      "Epoch [1/40], Step [269/469], D_loss: 0.3750, G_loss: 4.2467\n",
      "Epoch [1/40], Step [270/469], D_loss: 0.6174, G_loss: 4.0633\n",
      "Epoch [1/40], Step [271/469], D_loss: 0.8742, G_loss: 4.2005\n",
      "Epoch [1/40], Step [272/469], D_loss: 0.8020, G_loss: 4.4518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [273/469], D_loss: 0.5801, G_loss: 4.4014\n",
      "Epoch [1/40], Step [274/469], D_loss: 0.5288, G_loss: 4.2096\n",
      "Epoch [1/40], Step [275/469], D_loss: 0.4596, G_loss: 4.0351\n",
      "Epoch [1/40], Step [276/469], D_loss: 0.4877, G_loss: 4.0815\n",
      "Epoch [1/40], Step [277/469], D_loss: 0.3432, G_loss: 4.0138\n",
      "Epoch [1/40], Step [278/469], D_loss: 0.6400, G_loss: 3.9511\n",
      "Epoch [1/40], Step [279/469], D_loss: 0.6011, G_loss: 4.0969\n",
      "Epoch [1/40], Step [280/469], D_loss: 0.4717, G_loss: 4.2265\n",
      "Epoch [1/40], Step [281/469], D_loss: 0.5569, G_loss: 4.0501\n",
      "Epoch [1/40], Step [282/469], D_loss: 0.4205, G_loss: 4.0000\n",
      "Epoch [1/40], Step [283/469], D_loss: 0.4820, G_loss: 4.1121\n",
      "Epoch [1/40], Step [284/469], D_loss: 0.4812, G_loss: 4.1952\n",
      "Epoch [1/40], Step [285/469], D_loss: 0.4843, G_loss: 3.9468\n",
      "Epoch [1/40], Step [286/469], D_loss: 0.5904, G_loss: 4.0976\n",
      "Epoch [1/40], Step [287/469], D_loss: 0.6038, G_loss: 4.4915\n",
      "Epoch [1/40], Step [288/469], D_loss: 0.4320, G_loss: 4.4453\n",
      "Epoch [1/40], Step [289/469], D_loss: 0.2891, G_loss: 4.3571\n",
      "Epoch [1/40], Step [290/469], D_loss: 0.3861, G_loss: 3.5613\n",
      "Epoch [1/40], Step [291/469], D_loss: 1.2006, G_loss: 4.6289\n",
      "Epoch [1/40], Step [292/469], D_loss: 0.6174, G_loss: 5.1445\n",
      "Epoch [1/40], Step [293/469], D_loss: 0.2805, G_loss: 4.6384\n",
      "Epoch [1/40], Step [294/469], D_loss: 0.4107, G_loss: 3.8389\n",
      "Epoch [1/40], Step [295/469], D_loss: 0.3933, G_loss: 3.6546\n",
      "Epoch [1/40], Step [296/469], D_loss: 0.6598, G_loss: 4.3510\n",
      "Epoch [1/40], Step [297/469], D_loss: 0.4613, G_loss: 4.4472\n",
      "Epoch [1/40], Step [298/469], D_loss: 0.4018, G_loss: 4.2644\n",
      "Epoch [1/40], Step [299/469], D_loss: 0.4547, G_loss: 4.0173\n",
      "Epoch [1/40], Step [300/469], D_loss: 0.5314, G_loss: 3.9599\n",
      "Epoch [1/40], Step [301/469], D_loss: 0.5011, G_loss: 4.0097\n",
      "Epoch [1/40], Step [302/469], D_loss: 0.5511, G_loss: 4.0437\n",
      "Epoch [1/40], Step [303/469], D_loss: 0.7266, G_loss: 4.0500\n",
      "Epoch [1/40], Step [304/469], D_loss: 0.8008, G_loss: 4.3208\n",
      "Epoch [1/40], Step [305/469], D_loss: 0.5458, G_loss: 4.2860\n",
      "Epoch [1/40], Step [306/469], D_loss: 0.5764, G_loss: 3.9948\n",
      "Epoch [1/40], Step [307/469], D_loss: 0.4448, G_loss: 4.0350\n",
      "Epoch [1/40], Step [308/469], D_loss: 0.8506, G_loss: 4.0344\n",
      "Epoch [1/40], Step [309/469], D_loss: 0.7086, G_loss: 4.0479\n",
      "Epoch [1/40], Step [310/469], D_loss: 0.6810, G_loss: 4.1654\n",
      "Epoch [1/40], Step [311/469], D_loss: 0.4031, G_loss: 4.0950\n",
      "Epoch [1/40], Step [312/469], D_loss: 0.8689, G_loss: 3.7015\n",
      "Epoch [1/40], Step [313/469], D_loss: 1.0242, G_loss: 4.5141\n",
      "Epoch [1/40], Step [314/469], D_loss: 0.8617, G_loss: 4.6677\n",
      "Epoch [1/40], Step [315/469], D_loss: 0.6122, G_loss: 4.3384\n",
      "Epoch [1/40], Step [316/469], D_loss: 1.1111, G_loss: 4.1963\n",
      "Epoch [1/40], Step [317/469], D_loss: 1.0680, G_loss: 4.7294\n",
      "Epoch [1/40], Step [318/469], D_loss: 0.8683, G_loss: 4.7427\n",
      "Epoch [1/40], Step [319/469], D_loss: 1.0962, G_loss: 4.4129\n",
      "Epoch [1/40], Step [320/469], D_loss: 1.2662, G_loss: 5.2227\n",
      "Epoch [1/40], Step [321/469], D_loss: 1.3527, G_loss: 4.9660\n",
      "Epoch [1/40], Step [322/469], D_loss: 2.1010, G_loss: 4.5993\n",
      "Epoch [1/40], Step [323/469], D_loss: 1.9136, G_loss: 5.7919\n",
      "Epoch [1/40], Step [324/469], D_loss: 1.9388, G_loss: 4.9349\n",
      "Epoch [1/40], Step [325/469], D_loss: 2.5483, G_loss: 5.1866\n",
      "Epoch [1/40], Step [326/469], D_loss: 2.1929, G_loss: 5.1135\n",
      "Epoch [1/40], Step [327/469], D_loss: 2.2879, G_loss: 4.1980\n",
      "Epoch [1/40], Step [328/469], D_loss: 2.1859, G_loss: 5.6914\n",
      "Epoch [1/40], Step [329/469], D_loss: 2.7891, G_loss: 2.8308\n",
      "Epoch [1/40], Step [330/469], D_loss: 2.8633, G_loss: 6.5114\n",
      "Epoch [1/40], Step [331/469], D_loss: 2.5599, G_loss: 2.4826\n",
      "Epoch [1/40], Step [332/469], D_loss: 3.0769, G_loss: 5.8443\n",
      "Epoch [1/40], Step [333/469], D_loss: 2.1012, G_loss: 3.0764\n",
      "Epoch [1/40], Step [334/469], D_loss: 2.7182, G_loss: 4.2809\n",
      "Epoch [1/40], Step [335/469], D_loss: 2.1139, G_loss: 2.9182\n",
      "Epoch [1/40], Step [336/469], D_loss: 2.5608, G_loss: 5.0754\n",
      "Epoch [1/40], Step [337/469], D_loss: 1.8813, G_loss: 2.1205\n",
      "Epoch [1/40], Step [338/469], D_loss: 2.5089, G_loss: 5.6415\n",
      "Epoch [1/40], Step [339/469], D_loss: 2.3856, G_loss: 1.8945\n",
      "Epoch [1/40], Step [340/469], D_loss: 2.2268, G_loss: 5.8553\n",
      "Epoch [1/40], Step [341/469], D_loss: 2.2713, G_loss: 2.1394\n",
      "Epoch [1/40], Step [342/469], D_loss: 2.5724, G_loss: 4.6410\n",
      "Epoch [1/40], Step [343/469], D_loss: 1.8273, G_loss: 2.8255\n",
      "Epoch [1/40], Step [344/469], D_loss: 1.8940, G_loss: 3.8602\n",
      "Epoch [1/40], Step [345/469], D_loss: 1.2730, G_loss: 3.4655\n",
      "Epoch [1/40], Step [346/469], D_loss: 1.7951, G_loss: 2.7178\n",
      "Epoch [1/40], Step [347/469], D_loss: 1.7434, G_loss: 3.5442\n",
      "Epoch [1/40], Step [348/469], D_loss: 1.3899, G_loss: 3.1918\n",
      "Epoch [1/40], Step [349/469], D_loss: 1.7604, G_loss: 3.1163\n",
      "Epoch [1/40], Step [350/469], D_loss: 1.7456, G_loss: 3.2372\n",
      "Epoch [1/40], Step [351/469], D_loss: 1.5591, G_loss: 3.5337\n",
      "Epoch [1/40], Step [352/469], D_loss: 1.2902, G_loss: 3.6386\n",
      "Epoch [1/40], Step [353/469], D_loss: 1.3932, G_loss: 3.2422\n",
      "Epoch [1/40], Step [354/469], D_loss: 1.6485, G_loss: 3.4010\n",
      "Epoch [1/40], Step [355/469], D_loss: 1.4176, G_loss: 4.0031\n",
      "Epoch [1/40], Step [356/469], D_loss: 1.3814, G_loss: 3.4138\n",
      "Epoch [1/40], Step [357/469], D_loss: 1.6553, G_loss: 3.1455\n",
      "Epoch [1/40], Step [358/469], D_loss: 1.5282, G_loss: 3.9492\n",
      "Epoch [1/40], Step [359/469], D_loss: 1.1701, G_loss: 3.8435\n",
      "Epoch [1/40], Step [360/469], D_loss: 1.1797, G_loss: 3.3910\n",
      "Epoch [1/40], Step [361/469], D_loss: 1.2510, G_loss: 3.8133\n",
      "Epoch [1/40], Step [362/469], D_loss: 1.0616, G_loss: 3.5220\n",
      "Epoch [1/40], Step [363/469], D_loss: 1.0019, G_loss: 3.3653\n",
      "Epoch [1/40], Step [364/469], D_loss: 1.3813, G_loss: 4.6759\n",
      "Epoch [1/40], Step [365/469], D_loss: 1.1217, G_loss: 4.3227\n",
      "Epoch [1/40], Step [366/469], D_loss: 1.2353, G_loss: 3.4006\n",
      "Epoch [1/40], Step [367/469], D_loss: 1.0369, G_loss: 3.5894\n",
      "Epoch [1/40], Step [368/469], D_loss: 0.8768, G_loss: 4.0232\n",
      "Epoch [1/40], Step [369/469], D_loss: 0.9669, G_loss: 3.7121\n",
      "Epoch [1/40], Step [370/469], D_loss: 1.1058, G_loss: 3.9039\n",
      "Epoch [1/40], Step [371/469], D_loss: 0.9328, G_loss: 4.3987\n",
      "Epoch [1/40], Step [372/469], D_loss: 0.7591, G_loss: 4.1069\n",
      "Epoch [1/40], Step [373/469], D_loss: 0.7028, G_loss: 3.4905\n",
      "Epoch [1/40], Step [374/469], D_loss: 0.6853, G_loss: 3.3480\n",
      "Epoch [1/40], Step [375/469], D_loss: 0.6625, G_loss: 3.7637\n",
      "Epoch [1/40], Step [376/469], D_loss: 0.7009, G_loss: 4.3875\n",
      "Epoch [1/40], Step [377/469], D_loss: 1.1499, G_loss: 5.1194\n",
      "Epoch [1/40], Step [378/469], D_loss: 1.1794, G_loss: 4.3979\n",
      "Epoch [1/40], Step [379/469], D_loss: 1.0938, G_loss: 4.0916\n",
      "Epoch [1/40], Step [380/469], D_loss: 1.3245, G_loss: 5.2995\n",
      "Epoch [1/40], Step [381/469], D_loss: 0.9891, G_loss: 4.8627\n",
      "Epoch [1/40], Step [382/469], D_loss: 1.3166, G_loss: 5.0667\n",
      "Epoch [1/40], Step [383/469], D_loss: 0.9434, G_loss: 4.8677\n",
      "Epoch [1/40], Step [384/469], D_loss: 1.4117, G_loss: 4.0797\n",
      "Epoch [1/40], Step [385/469], D_loss: 1.5968, G_loss: 5.3163\n",
      "Epoch [1/40], Step [386/469], D_loss: 1.3165, G_loss: 4.4715\n",
      "Epoch [1/40], Step [387/469], D_loss: 1.5027, G_loss: 4.7070\n",
      "Epoch [1/40], Step [388/469], D_loss: 1.3889, G_loss: 4.2921\n",
      "Epoch [1/40], Step [389/469], D_loss: 1.3186, G_loss: 3.9573\n",
      "Epoch [1/40], Step [390/469], D_loss: 1.1112, G_loss: 4.4492\n",
      "Epoch [1/40], Step [391/469], D_loss: 1.3478, G_loss: 3.6105\n",
      "Epoch [1/40], Step [392/469], D_loss: 0.9525, G_loss: 4.1497\n",
      "Epoch [1/40], Step [393/469], D_loss: 0.8871, G_loss: 4.4756\n",
      "Epoch [1/40], Step [394/469], D_loss: 1.0113, G_loss: 3.8419\n",
      "Epoch [1/40], Step [395/469], D_loss: 0.8658, G_loss: 3.7507\n",
      "Epoch [1/40], Step [396/469], D_loss: 1.3323, G_loss: 3.6779\n",
      "Epoch [1/40], Step [397/469], D_loss: 1.2395, G_loss: 4.0904\n",
      "Epoch [1/40], Step [398/469], D_loss: 0.7854, G_loss: 4.3231\n",
      "Epoch [1/40], Step [399/469], D_loss: 1.2186, G_loss: 3.6776\n",
      "Epoch [1/40], Step [400/469], D_loss: 1.5959, G_loss: 3.6715\n",
      "Epoch [1/40], Step [401/469], D_loss: 1.3741, G_loss: 4.0558\n",
      "Epoch [1/40], Step [402/469], D_loss: 1.0929, G_loss: 3.8982\n",
      "Epoch [1/40], Step [403/469], D_loss: 1.3363, G_loss: 3.8828\n",
      "Epoch [1/40], Step [404/469], D_loss: 1.1458, G_loss: 3.9752\n",
      "Epoch [1/40], Step [405/469], D_loss: 1.1459, G_loss: 3.6978\n",
      "Epoch [1/40], Step [406/469], D_loss: 1.4095, G_loss: 4.2282\n",
      "Epoch [1/40], Step [407/469], D_loss: 1.5177, G_loss: 3.6677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Step [408/469], D_loss: 1.5585, G_loss: 3.7590\n",
      "Epoch [1/40], Step [409/469], D_loss: 1.2965, G_loss: 4.2483\n",
      "Epoch [1/40], Step [410/469], D_loss: 1.6094, G_loss: 3.6255\n",
      "Epoch [1/40], Step [411/469], D_loss: 1.4084, G_loss: 4.1550\n",
      "Epoch [1/40], Step [412/469], D_loss: 1.4256, G_loss: 4.0311\n",
      "Epoch [1/40], Step [413/469], D_loss: 1.2014, G_loss: 4.4309\n",
      "Epoch [1/40], Step [414/469], D_loss: 1.2458, G_loss: 3.3346\n",
      "Epoch [1/40], Step [415/469], D_loss: 1.4048, G_loss: 4.0348\n",
      "Epoch [1/40], Step [416/469], D_loss: 1.1131, G_loss: 3.4229\n",
      "Epoch [1/40], Step [417/469], D_loss: 1.3749, G_loss: 5.2655\n",
      "Epoch [1/40], Step [418/469], D_loss: 1.1387, G_loss: 3.8872\n",
      "Epoch [1/40], Step [419/469], D_loss: 1.1037, G_loss: 3.1650\n",
      "Epoch [1/40], Step [420/469], D_loss: 1.3726, G_loss: 4.5693\n",
      "Epoch [1/40], Step [421/469], D_loss: 1.4042, G_loss: 3.5933\n",
      "Epoch [1/40], Step [422/469], D_loss: 1.4185, G_loss: 3.6190\n",
      "Epoch [1/40], Step [423/469], D_loss: 1.0793, G_loss: 3.3859\n",
      "Epoch [1/40], Step [424/469], D_loss: 1.2684, G_loss: 3.5185\n",
      "Epoch [1/40], Step [425/469], D_loss: 1.1770, G_loss: 3.6201\n",
      "Epoch [1/40], Step [426/469], D_loss: 1.2670, G_loss: 4.2037\n",
      "Epoch [1/40], Step [427/469], D_loss: 0.8625, G_loss: 4.3717\n",
      "Epoch [1/40], Step [428/469], D_loss: 1.6789, G_loss: 2.9831\n",
      "Epoch [1/40], Step [429/469], D_loss: 1.6808, G_loss: 4.7602\n",
      "Epoch [1/40], Step [430/469], D_loss: 1.4019, G_loss: 3.7045\n",
      "Epoch [1/40], Step [431/469], D_loss: 1.4344, G_loss: 4.1028\n",
      "Epoch [1/40], Step [432/469], D_loss: 1.7280, G_loss: 4.2561\n",
      "Epoch [1/40], Step [433/469], D_loss: 1.8240, G_loss: 3.8278\n",
      "Epoch [1/40], Step [434/469], D_loss: 1.6803, G_loss: 4.3707\n",
      "Epoch [1/40], Step [435/469], D_loss: 1.6141, G_loss: 3.8114\n",
      "Epoch [1/40], Step [436/469], D_loss: 1.9084, G_loss: 4.4634\n",
      "Epoch [1/40], Step [437/469], D_loss: 1.8984, G_loss: 4.1811\n",
      "Epoch [1/40], Step [438/469], D_loss: 2.0186, G_loss: 3.9518\n",
      "Epoch [1/40], Step [439/469], D_loss: 1.7431, G_loss: 3.9551\n",
      "Epoch [1/40], Step [440/469], D_loss: 2.2870, G_loss: 3.8596\n",
      "Epoch [1/40], Step [441/469], D_loss: 1.7172, G_loss: 4.3923\n",
      "Epoch [1/40], Step [442/469], D_loss: 1.3699, G_loss: 3.9815\n",
      "Epoch [1/40], Step [443/469], D_loss: 1.8789, G_loss: 3.3517\n",
      "Epoch [1/40], Step [444/469], D_loss: 1.7508, G_loss: 4.7640\n",
      "Epoch [1/40], Step [445/469], D_loss: 1.5915, G_loss: 4.0725\n",
      "Epoch [1/40], Step [446/469], D_loss: 2.3873, G_loss: 4.8890\n",
      "Epoch [1/40], Step [447/469], D_loss: 2.3541, G_loss: 4.7693\n",
      "Epoch [1/40], Step [448/469], D_loss: 2.3697, G_loss: 3.2260\n",
      "Epoch [1/40], Step [449/469], D_loss: 2.9020, G_loss: 4.9693\n",
      "Epoch [1/40], Step [450/469], D_loss: 2.0127, G_loss: 3.4600\n",
      "Epoch [1/40], Step [451/469], D_loss: 1.9015, G_loss: 3.5306\n",
      "Epoch [1/40], Step [452/469], D_loss: 1.4302, G_loss: 4.6046\n",
      "Epoch [1/40], Step [453/469], D_loss: 1.6039, G_loss: 3.3286\n",
      "Epoch [1/40], Step [454/469], D_loss: 1.4459, G_loss: 4.9937\n",
      "Epoch [1/40], Step [455/469], D_loss: 1.4247, G_loss: 3.5477\n",
      "Epoch [1/40], Step [456/469], D_loss: 1.7002, G_loss: 4.4677\n",
      "Epoch [1/40], Step [457/469], D_loss: 1.4917, G_loss: 3.7060\n",
      "Epoch [1/40], Step [458/469], D_loss: 1.3736, G_loss: 3.7208\n",
      "Epoch [1/40], Step [459/469], D_loss: 1.1836, G_loss: 3.4855\n",
      "Epoch [1/40], Step [460/469], D_loss: 0.8365, G_loss: 3.8571\n",
      "Epoch [1/40], Step [461/469], D_loss: 0.8566, G_loss: 3.2585\n",
      "Epoch [1/40], Step [462/469], D_loss: 1.1381, G_loss: 2.6086\n",
      "Epoch [1/40], Step [463/469], D_loss: 0.7837, G_loss: 3.8904\n",
      "Epoch [1/40], Step [464/469], D_loss: 0.7353, G_loss: 3.3735\n",
      "Epoch [1/40], Step [465/469], D_loss: 0.6282, G_loss: 2.9488\n",
      "Epoch [1/40], Step [466/469], D_loss: 0.6509, G_loss: 3.9372\n",
      "Epoch [1/40], Step [467/469], D_loss: 0.7736, G_loss: 3.3989\n",
      "Epoch [1/40], Step [468/469], D_loss: 0.7456, G_loss: 3.2819\n",
      "Epoch [1/40], Step [469/469], D_loss: 0.7581, G_loss: 3.2863\n",
      "Epoch [2/40], Step [1/469], D_loss: 0.8401, G_loss: 3.2492\n",
      "Epoch [2/40], Step [2/469], D_loss: 0.7473, G_loss: 3.1598\n",
      "Epoch [2/40], Step [3/469], D_loss: 0.6623, G_loss: 3.4928\n",
      "Epoch [2/40], Step [4/469], D_loss: 0.9648, G_loss: 2.9504\n",
      "Epoch [2/40], Step [5/469], D_loss: 1.0147, G_loss: 3.8825\n",
      "Epoch [2/40], Step [6/469], D_loss: 1.0024, G_loss: 3.5733\n",
      "Epoch [2/40], Step [7/469], D_loss: 1.3182, G_loss: 4.1283\n",
      "Epoch [2/40], Step [8/469], D_loss: 0.8345, G_loss: 3.8590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b2d83faa9dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mD_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mG_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MNIST image generation using DCGAN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "# Parameters\n",
    "image_size = 64\n",
    "G_input_dim = 100\n",
    "G_output_dim = 1\n",
    "D_input_dim = 1\n",
    "D_output_dim = 1\n",
    "num_filters = [1024, 512, 256, 128]\n",
    "\n",
    "learning_rate = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "batch_size = 128\n",
    "num_epochs = 40\n",
    "data_dir = 'mnist_data'\n",
    "save_dir = 'MNIST_DCGAN_results/'\n",
    "\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, ), std=(0.5,))])\n",
    "\n",
    "mnist_data = dsets.MNIST(root=data_dir,\n",
    "                         train=True,\n",
    "                         transform=transform,\n",
    "                         download=False)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "# De-normalization\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "# Generator model\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layer = torch.nn.Sequential()\n",
    "        for i in range(len(num_filters)):\n",
    "            # Deconvolutional layer\n",
    "            if i == 0:\n",
    "                deconv = torch.nn.ConvTranspose2d(input_dim, num_filters[i], kernel_size=4, stride=1, padding=0)\n",
    "            else:\n",
    "                deconv = torch.nn.ConvTranspose2d(num_filters[i-1], num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            deconv_name = 'deconv' + str(i + 1)\n",
    "            self.hidden_layer.add_module(deconv_name, deconv)\n",
    "\n",
    "            # Initializer\n",
    "            torch.nn.init.normal_(deconv.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(deconv.bias, 0.0)\n",
    "\n",
    "            # Batch normalization\n",
    "            bn_name = 'bn' + str(i + 1)\n",
    "            self.hidden_layer.add_module(bn_name, torch.nn.BatchNorm2d(num_filters[i]))\n",
    "\n",
    "            # Activation\n",
    "            act_name = 'act' + str(i + 1)\n",
    "            self.hidden_layer.add_module(act_name, torch.nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = torch.nn.Sequential()\n",
    "        # Deconvolutional layer\n",
    "        out = torch.nn.ConvTranspose2d(num_filters[i], output_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.output_layer.add_module('out', out)\n",
    "        # Initializer\n",
    "        torch.nn.init.normal_(out.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(out.bias, 0.0)\n",
    "        # Activation\n",
    "        self.output_layer.add_module('act', torch.nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.hidden_layer(x)\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_filters, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layer = torch.nn.Sequential()\n",
    "        for i in range(len(num_filters)):\n",
    "            # Convolutional layer\n",
    "            if i == 0:\n",
    "                conv = torch.nn.Conv2d(input_dim, num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "            else:\n",
    "                conv = torch.nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            conv_name = 'conv' + str(i + 1)\n",
    "            self.hidden_layer.add_module(conv_name, conv)\n",
    "\n",
    "            # Initializer\n",
    "            torch.nn.init.normal_(conv.weight, mean=0.0, std=0.02)\n",
    "            torch.nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "            # Batch normalization\n",
    "            if i != 0:\n",
    "                bn_name = 'bn' + str(i + 1)\n",
    "                self.hidden_layer.add_module(bn_name, torch.nn.BatchNorm2d(num_filters[i]))\n",
    "\n",
    "            # Activation\n",
    "            act_name = 'act' + str(i + 1)\n",
    "            self.hidden_layer.add_module(act_name, torch.nn.LeakyReLU(0.2))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = torch.nn.Sequential()\n",
    "        # Convolutional layer\n",
    "        out = torch.nn.Conv2d(num_filters[i], output_dim, kernel_size=4, stride=1, padding=0)\n",
    "        self.output_layer.add_module('out', out)\n",
    "        # Initializer\n",
    "        torch.nn.init.normal_(out.weight, mean=0.0, std=0.02)\n",
    "        torch.nn.init.constant_(out.bias, 0.0)\n",
    "        # Activation\n",
    "        self.output_layer.add_module('act', torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.hidden_layer(x)\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Plot losses\n",
    "def plot_loss(d_losses, g_losses, num_epoch, save=False, save_dir='MNIST_DCGAN_results/', show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, num_epochs)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch + 1))\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'MNIST_DCGAN_losses_epoch_{:d}'.format(num_epoch + 1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_result(generator, noise, num_epoch, save=False, save_dir='MNIST_DCGAN_results/', show=False, fig_size=(5, 5)):\n",
    "    generator.eval()\n",
    "\n",
    "    noise = Variable(noise.cuda())\n",
    "    gen_image = generator(noise)\n",
    "    gen_image = denorm(gen_image)\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    n_rows = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    n_cols = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\n",
    "    for ax, img in zip(axes.flatten(), gen_image):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box')\n",
    "        ax.imshow(img.cpu().data.view(image_size, image_size).numpy(), cmap='gray', aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    title = 'Epoch {0}'.format(num_epoch+1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'MNIST_DCGAN_epoch_{:d}'.format(num_epoch+1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Models\n",
    "G = Generator(G_input_dim, num_filters, G_output_dim)\n",
    "D = Discriminator(D_input_dim, num_filters[::-1], D_output_dim)\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=betas)\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate*0.1, betas=betas)\n",
    "\n",
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "# Fixed noise for test\n",
    "num_test_samples = 5*5\n",
    "fixed_noise = torch.randn(num_test_samples, G_input_dim).view(-1, G_input_dim, 1, 1)\n",
    "\n",
    "train_D=True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # minibatch training\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "\n",
    "        # image data\n",
    "        mini_batch = images.size()[0]\n",
    "        x_ = Variable(images.cuda())\n",
    "\n",
    "        # labels\n",
    "        y_real_ = Variable(torch.ones(mini_batch).cuda())\n",
    "        y_fake_ = Variable(torch.zeros(mini_batch).cuda())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_).squeeze()\n",
    "        # print(D_real_decision, y_real_)\n",
    "        D_real_loss = criterion(D_real_decision, y_real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        z_ = torch.randn(mini_batch, G_input_dim).view(-1, G_input_dim, 1, 1)\n",
    "        z_ = Variable(z_.cuda())\n",
    "        gen_image = G(z_)\n",
    "\n",
    "        D_fake_decision = D(gen_image).squeeze()\n",
    "        D_fake_loss = criterion(D_fake_decision, y_fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        D.zero_grad()\n",
    "        #if train_D:\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        z_ = torch.randn(mini_batch, G_input_dim).view(-1, G_input_dim, 1, 1)\n",
    "        z_ = Variable(z_.cuda())\n",
    "        gen_image = G(z_)\n",
    "\n",
    "        D_fake_decision = D(gen_image).squeeze()\n",
    "        G_loss = criterion(D_fake_decision, y_real_)\n",
    "\n",
    "        # Back propagation\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "        if D_loss.item()/(G_loss.item()+1e-8)<0.1:\n",
    "            train_D=False\n",
    "        else:\n",
    "            train_D=True\n",
    "\n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "              % (epoch+1, num_epochs, i+1, len(data_loader), D_loss.item(), G_loss.item()))\n",
    "\n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "    plot_loss(D_avg_losses, G_avg_losses, epoch, save=True)\n",
    "\n",
    "    # Show result for fixed noise\n",
    "    plot_result(G, fixed_noise, epoch, save=True, fig_size=(5, 5))\n",
    "\n",
    "# Make gif\n",
    "loss_plots = []\n",
    "gen_image_plots = []\n",
    "for epoch in range(num_epochs):\n",
    "    # plot for generating gif\n",
    "    save_fn1 = save_dir + 'MNIST_DCGAN_losses_epoch_{:d}'.format(epoch + 1) + '.png'\n",
    "    loss_plots.append(imageio.imread(save_fn1))\n",
    "\n",
    "    save_fn2 = save_dir + 'MNIST_DCGAN_epoch_{:d}'.format(epoch + 1) + '.png'\n",
    "    gen_image_plots.append(imageio.imread(save_fn2))\n",
    "\n",
    "imageio.mimsave(save_dir + 'MNIST_DCGAN_losses_epochs_{:d}'.format(num_epochs) + '.gif', loss_plots, fps=5)\n",
    "imageio.mimsave(save_dir + 'MNIST_DCGAN_epochs_{:d}'.format(num_epochs) + '.gif', gen_image_plots, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
